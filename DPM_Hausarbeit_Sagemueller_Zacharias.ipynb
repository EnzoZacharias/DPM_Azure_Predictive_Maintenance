{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Durchführung eines Diagnose- und Predicitve Maintenance-Projekts (Lea Sagemüller, Enzo Zacharias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Rahmen des Wahlmoduls Diagnose und Predictive Maintenance im fünften Semester des Studiengangs Digitale Technologien wurde als Modulprüfung eine Hausarbeit verfasst, die ein Projekt im Diagnose- und Predictive Maintenance-Bereich beinhaltet. Aus den behandelten Kapiteln der Lehrveranstaltung sollten jeweils Inhalte aufgegriffen werden und auf einen ausgewählten Datensatz angewendet werden. Der Ablauf des Projektes unterteilt sich in die Datenbeschreibung, die Datenvorverarbeitung, die Anomalieerkennung, Predicitve Maintenance sowie die Diagnose und Fehlerursachenerkennung. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswahl des Datensatzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Durchführung des Projektes wurde ein passender Datensatz ausgewählt, an welchem die oben genannten Schritte durchgeführt werden können. Der Microsoft Azure Predictive Maintenance-Datensatz auf Kaggle bietet dafür eine umfassende Grundlage. Er umfasst verschiedene Datenquellen, darunter Sensordaten, Fehlerprotokolle, Wartungsinformationen sowie Maschinendaten. Ziel des Datensatzes ist es, Muster und Zusammenhänge zu erkennen, die dabei helfen, Ausfälle frühzeitig vorherzusagen und die Wartungsplanung zu optimieren. \n",
    "Die Daten erhalten zeitlich gestempelte Sensormessungen, die den Zustand und Betrieb von Maschinen überwachen, beispielsweise Temperatur- oder Druckwerte. Ergänzend dazu dokumentieren Fehlerprotokolle aufgetretene Störungen, während Wartungslogs Informationen über geplante oder ungeplante Instandhaltungsmaßnahmen bereitstellen. Stammdaten zu den Maschinen, wie Modelltyp oder Alter, sowie Details zu tatsächlichen Ausfällen, einschließlich Ausfalltyp und Zeitstempel, sind ebenfalls Teil des Datensatzes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen der Datensätze und Zusammenführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import der Haupttabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'..\\data\\PdM_telemetry.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst wird die Haupttabelle \"PdM_telemetry.csv\" importiert, im Folgenden die vier zusätzlichen Tabellen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusatz-Tabellen importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_errors = pd.read_csv(r'..\\data\\PdM_errors.csv')\n",
    "df_failures = pd.read_csv(r'..\\data\\PdM_failures.csv')\n",
    "df_maint = pd.read_csv(r'..\\data\\PdM_maint.csv')\n",
    "df_machines = pd.read_csv(r'..\\data\\PdM_machines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausgabe der einzelnen Datensätze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun werden jeweils die ersten fünf Einträge aus den erstellten Dataframes der einzelnen Datensätze ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Errors\n",
    "df_errors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Failures\n",
    "df_failures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe von Maint\n",
    "df_maint.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe von Machines\n",
    "df_machines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier weiß ich nicht mehr, was genau der Plan war:\n",
    "\n",
    "Hier könnte man diese Tabelle exemplarisch als Beispiel aufzeigen --> Danach zusammenführen in einem Dataframe in Bezug auf machineID und datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Definieren des Dateipfades\n",
    "directory = r'../data'\n",
    "files = [r'PdM_errors.csv', r'PdM_failures.csv', r'PdM_machines.csv', r'PdM_maint.csv', r'PdM_telemetry.csv']\n",
    "\n",
    "dfs = {}\n",
    "for file in files:\n",
    "    filepath = os.path.join(directory, file)\n",
    "    df_name = file.split('.')[0]\n",
    "    dfs[df_name] = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es können nun die einzelnen Dataframes zu einem gemeinsamen Dataframe in Bezug auf die Spalten \"datetime\" und \"machineID\" zusammengeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = dfs['PdM_telemetry']\n",
    "merged_df = merged_df.merge(dfs['PdM_errors'], on=['datetime', 'machineID'], how='left')\n",
    "merged_df = merged_df.merge(dfs['PdM_failures'], on=['datetime', 'machineID'], how='left')\n",
    "merged_df = merged_df.merge(dfs['PdM_maint'], on=['datetime', 'machineID'], how='left')\n",
    "merged_df = merged_df.merge(dfs['PdM_machines'], on=['machineID'], how='left')\n",
    "\n",
    "merged_df.head()\n",
    "\n",
    "# Speichern des zusammengeführten Dataframes\n",
    "merged_df.to_csv(r'../data/merged_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenbeschreibung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden wird der im CSV-Format vorliegende zusammengeführte Datensatz importiert und in einen Dataframe eingelesen. Ebenfalls in einen Dataframe eingelesen wird der Datensatz, der die Fehlerinformationen enthält. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'../data/PdM_merged.csv')\n",
    "\n",
    "df_error = pd.read_csv(r'../data/PdM_errors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anschauen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es werden die ersten fünf Einträge des eingelesenen Datensatzes angezeigt, um mehr über die Struktur der Daten zu erfahren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Beschreibung der Daten ist hilfreich, um Informationen wie Minimal- und Maximalwert sowie Mittelwert oder Standardabweichung zu untersuchen und dort ggf. schon erste Auffälligkeiten erkennen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es werden nun die einzelnen Spalten mit ihren zugehörigen Datentypen angezeigt. Dabei fällt direkt auf, dass die Spalte \"datetime\" nicht im richtigen Format vorliegt und im weiteren Verlauf angepasst werden muss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotten aller Messwerte (pro Tag / pro Monat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige die Spannung (voltage), Drehzahl (rotate), den Druck (pressure) und die Vibration (vibration) in jeweils einem Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.dates import DateFormatter\n",
    "date_format = DateFormatter(\"%H:%M\")\n",
    "\n",
    "# Filtern der Daten für die ersten 24 Stunden\n",
    "df_filtered = df[(df['datetime'] >= '2015-01-01 06:00:00') & (df['datetime'] < '2015-01-02 06:00:00') & (df['machineID'] == 1)]\n",
    "fig, axs = plt.subplots(4, 1, figsize=(12, 24), sharex=True)\n",
    "\n",
    "# Plot für Spannung (voltage)\n",
    "sns.lineplot(x='datetime', y='volt', data=df_filtered, ax=axs[0], color='b')\n",
    "axs[0].set_title('Voltage per Machine per Day (First 24 Hours)')\n",
    "axs[0].set_ylabel('Voltage')\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot für Drehzahl (rotate)\n",
    "sns.lineplot(x='datetime', y='rotate', data=df_filtered, ax=axs[1], color='g')\n",
    "axs[1].set_title('Rotate per Machine per Day (First 24 Hours)')\n",
    "axs[1].set_ylabel('Rotate')\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Plot für Druck (pressure)\n",
    "sns.lineplot(x='datetime', y='pressure', data=df_filtered, ax=axs[2], color='r')\n",
    "axs[2].set_title('Pressure per Machine per Day (First 24 Hours)')\n",
    "axs[2].set_ylabel('Pressure')\n",
    "axs[2].grid(True)\n",
    "\n",
    "# Plot für Vibration (vibration) \n",
    "sns.lineplot(x='datetime', y='vibration', data=df_filtered, ax=axs[3], color='m')\n",
    "axs[3].set_title('Vibration per Machine per Day (First 24 Hours)')\n",
    "axs[3].set_ylabel('Vibration')\n",
    "axs[3].set_xlabel('Datetime')\n",
    "axs[3].grid(True)\n",
    "\n",
    "# Formatieren der x-Achse, um nur die Stunden anzuzeigen\n",
    "for ax in axs:\n",
    "    ax.xaxis.set_major_formatter(date_format)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Code werden für die erste Maschine (\"machineID\" = 1) die Messdaten für Spannung (Voltage), Drehzahl (Rotate), Druck (Pressure) und Vibration in separaten Diagrammen visualisiert. Die Daten werden auf einen Zeitraum von 24 Stunden (vom 1. Januar 2015, 06:00 Uhr bis zum 2. Januar 2015, 06:00 Uhr) gefiltert. Dabei zeigt die x-Achse nur den Zeitstempel, formatiert als Uhrzeit (Stunden:Minuten), da nur auf 24 Stunden gefiltert wurde. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige die Spannung (voltage), Drehzahl (rotate), den Druck (pressure) und die Vibration (vibration) in jeweils einem Plot\n",
    "\n",
    "from matplotlib.dates import DateFormatter\n",
    "date_format = DateFormatter(\"%d-%m-%Y\")\n",
    "\n",
    "# Filtern der Daten für die ersten 31 Tage\n",
    "df_filtered = df[(df['datetime'] >= '2015-01-01 06:00:00') & (df['datetime'] < '2015-01-31 23:59:00') & (df['machineID'] == 1)]\n",
    "fig, axs = plt.subplots(4, 1, figsize=(12, 24), sharex=True)\n",
    "\n",
    "# Plot für Spannung (voltage)\n",
    "sns.lineplot(x='datetime', y='volt', data=df_filtered, ax=axs[0], color='b')\n",
    "axs[0].set_title('Voltage per Machine per Day (First 31 Days)')\n",
    "axs[0].set_ylabel('Voltage')\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot für Drehzahl (rotate)\n",
    "sns.lineplot(x='datetime', y='rotate', data=df_filtered, ax=axs[1], color='g')\n",
    "axs[1].set_title('Rotate per Machine per Day (First 31 Days)')\n",
    "axs[1].set_ylabel('Rotate')\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Plot für Druck (pressure)\n",
    "sns.lineplot(x='datetime', y='pressure', data=df_filtered, ax=axs[2], color='r')\n",
    "axs[2].set_title('Pressure per Machine per Day (First 31 Days)')\n",
    "axs[2].set_ylabel('Pressure')\n",
    "axs[2].grid(True)\n",
    "\n",
    "# Plot für Vibration (vibration)\n",
    "sns.lineplot(x='datetime', y='vibration', data=df_filtered, ax=axs[3], color='m')\n",
    "axs[3].set_title('Vibration per Machine per Day (First 31 Days)')\n",
    "axs[3].set_ylabel('Vibration')\n",
    "axs[3].set_xlabel('Datetime')\n",
    "axs[3].grid(True)\n",
    "\n",
    "# Formatieren der x-Achse, um nur die Tage anzuzeigen \n",
    "for ax in axs:\n",
    "    ax.xaxis.set_major_formatter(date_format)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Code ist fast identisch zum vorherigen mit dem Unterschied, dass nun ein Zeitraum von 31 Tagen betrachtet wird und demnach auch die x-Achse in Tagen statt in Stunden und Minuten angezeit werden soll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anzeigen der größten und kleinsten Ausprägungen pro Attribut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_timestamp = df['datetime'].min()\n",
    "max_timestamp = df['datetime'].max()\n",
    "\n",
    "print(f'Minimum timestamp: {min_timestamp}')\n",
    "print(f'Maximum timestamp: {max_timestamp}\\n')\n",
    "\n",
    "min_volt = df['volt'].min()\n",
    "max_volt = df['volt'].max()\n",
    "\n",
    "print(f'Minimum voltage: {min_volt:.2f}')\n",
    "print(f'Maximum voltage: {max_volt:.2f}\\n')\n",
    "\n",
    "min_rotate = df['rotate'].min()\n",
    "max_rotate = df['rotate'].max()\n",
    "\n",
    "print(f'Minimum rotate: {min_rotate:.2f}')\n",
    "print(f'Maximum rotate: {max_rotate:.2f}\\n')\n",
    "\n",
    "min_pressure = df['pressure'].min()\n",
    "max_pressure = df['pressure'].max()\n",
    "\n",
    "print(f'Minimum pressure: {min_pressure:.2f}')\n",
    "print(f'Maximum pressure: {max_pressure:.2f}\\n')\n",
    "\n",
    "min_vibration = df['vibration'].min()\n",
    "max_vibration = df['vibration'].max()\n",
    "\n",
    "print(f'Minimum vibration: {min_vibration:.2f}')\n",
    "print(f'Maximum vibration: {max_vibration:.2f}\\n')\n",
    "\n",
    "min_age = df['age'].min()\n",
    "max_age = df['age'].max()\n",
    "\n",
    "print(f'Minimum age: {min_age:.2f}')\n",
    "print(f'Maximum age: {max_age:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es werden nun die minimalen und maximalen Werte für verschiedene Attribute im Dataframe berechnet und ausgegeben.Zunächst wird der früheste und späteste Zeitstempel der Datensätze ermittelt und angezeigt. Danach werden die minimalen und maximalen Werte der numerischen Variablen volt (Spannung), rotate (Drehzahl), pressure (Druck), vibration (Vibration) sowie age (Alter der Maschine) berechnet. Diese Analyse gibt einen Überblick über die Verteilung und die Spannweite der Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untersuchung der Korrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formal werden alle Strings von errorID, failure und comp in int-Werte umgewandelt (beachtet werden die NaN-Werte dabei als 0, sonst als 1)\n",
    "db_bfr = df.copy()\n",
    "db_bfr['errorID'] = db_bfr['errorID'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "db_bfr['failure'] = db_bfr['failure'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "db_bfr['comp'] = db_bfr['comp'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "correlation = db_bfr[['volt', 'rotate', 'pressure', 'vibration', 'age', 'errorID', 'failure', 'comp']].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Untersuchung der Korrelation werden zunächst die Spalten errorID, failure und comp in numerische Werte umgewandelt, wobei NaN-Werte als 0 und alle anderen als 1 interpretiert werden. Anschließend wird eine Korrelationsmatrix berechnet, die die linearen Zusammenhänge zwischen den Variablen volt, rotate, pressure, vibration, age, errorID, failure und comp darstellt. Diese wird mit einer Heatmap visualisiert. Die Darstellung ermöglicht es, potenzielle Zusammenhänge und Einflussfaktoren auf Fehler und Ausfälle zu erkennen. Dabei ist in diesem Fall eine Korrelation zwischen failure und comp erkennbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autokorrelation von volt, rotate, pressure und vibration (Korrelation mit sich selbst in Bezug auf die Zeit)\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=(12, 24))\n",
    "\n",
    "# Autokorrelation von volt\n",
    "plot_acf(df['volt'], lags=100, ax=axs[0])\n",
    "axs[0].set_title('Autocorrelation of Voltage')\n",
    "\n",
    "# Autokorrelation von rotate\n",
    "plot_acf(df['rotate'], lags=100, ax=axs[1])\n",
    "axs[1].set_title('Autocorrelation of Rotate')\n",
    "\n",
    "# Autokorrelation von pressure\n",
    "plot_acf(df['pressure'], lags=100, ax=axs[2])\n",
    "axs[2].set_title('Autocorrelation of Pressure')\n",
    "\n",
    "# Autokorrelation von vibration\n",
    "plot_acf(df['vibration'], lags=100, ax=axs[3])\n",
    "axs[3].set_title('Autocorrelation of Vibration')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend wird die Autokorrelation für die Variablen volt (Spannung), rotate (Drehzahl), pressure (Druck) und vibration (Vibration) berechnet und in separaten Plots dargestellt. Autokorrelation beschreibt, wie stark eine Zeitreihe mit sich selbst in unterschiedlichen Zeitabständen (Lags) korreliert.\n",
    "\n",
    "Jeder Plot zeigt die Autokorrelation für bis zu 100 Lags, also Zeitpunkte, die jeweils um eine bestimmte Anzahl von Intervallen voneinander entfernt sind. Ein Wert nahe 1 deutet auf eine starke positive Korrelation hin, ein Wert nahe -1 auf eine starke negative Korrelation. Werte nahe 0 zeigen keine Korrelation. Die Plots ermöglichen es, Muster oder saisonale Abhängigkeiten in den Zeitreihen zu identifizieren.\n",
    "\n",
    "In allen vier Fällen ist eine schwache Korrelation von höchstens 0,2 zu erkennen.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausfälle pro Maschine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Anzahl Ausfälle pro Maschine und errorID als DataFrame erstellen und sortieren\n",
    "failure_count = df_error.groupby(['machineID', 'errorID']).size().unstack(fill_value=0)\n",
    "failure_count_sorted = failure_count.sum(axis=1).sort_values(ascending=False)\n",
    "\n",
    "# Figur mit angepasster Größe erstellen\n",
    "plt.figure(figsize=(15, 10))  \n",
    "\n",
    "# Balkendiagramm erstellen\n",
    "bottom = np.zeros(len(failure_count_sorted))\n",
    "colors = ['#2E86C1', '#E74C3C', '#F1C40F', '#2ECC71', '#9B59B6']  \n",
    "\n",
    "for i, errorID in enumerate(failure_count.columns):\n",
    "    values = failure_count.loc[failure_count_sorted.index, errorID]\n",
    "    plt.bar(range(len(failure_count_sorted)), values, bottom=bottom, color=colors[i % len(colors)], label=errorID)\n",
    "    bottom += values\n",
    "\n",
    "# X-Achsen-Labels mit den entsprechenden Maschinen-IDs\n",
    "plt.xticks(range(len(failure_count_sorted)), failure_count_sorted.index, \n",
    "           rotation=90,  \n",
    "           ha='center',  \n",
    "           fontsize=8)   \n",
    "\n",
    "plt.title('Fehler pro Maschine', fontsize=14, pad=20)\n",
    "plt.xlabel('Maschinen ID', fontsize=12, labelpad=10)  \n",
    "plt.ylabel('Anzahl Fehler', fontsize=12)\n",
    "\n",
    "# Gitter hinzufügen\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Ränder anpassen\n",
    "plt.margins(x=0.01)\n",
    "\n",
    "plt.subplots_adjust(bottom=0.2)  \n",
    "\n",
    "# Legende hinzufügen\n",
    "plt.legend(title='Error ID')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Ausfälle pro Maschine untersuchen zu können, wird ein gestapeltes Balkendiagramm erstellt, das die Anzahl der Fehler pro Maschine und Fehler-ID darstellt. Zunächst werden die Fehlerdaten nach machineID und errorID gruppiert und die Anzahl der Fehler berechnet, wobei fehlende Werte mit 0 aufgefüllt werden. Anschließend werden die Maschinen nach der Gesamtanzahl ihrer Fehler sortiert. Im Diagramm repräsentiert jeder Balken die Gesamtanzahl der Fehler pro Maschine. Das Diagramm ermöglicht es, die fehleranfälligsten Maschinen sowie die Verteilung der Fehlerarten zu erkennen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anzahl an Fehlerklassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Balkendiagramm erstellen mit x = Error-Klasse und y = Anzahl der Fehler\n",
    "sns.countplot(x='errorID', data=df_error, order=df_error['errorID'].value_counts().index, palette=colors)\n",
    "\n",
    "plt.title('Anzahl der Fehler', fontsize=14, pad=20)\n",
    "plt.ylabel('Anzahl der Fehler', fontsize=12)\n",
    "plt.xlabel('Error ID', fontsize=12)\n",
    "\n",
    "plt.margins(x=0.01)\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mittels eines Balkendiagramms wird die Verteilung der Fehlerarten visualisiert. Dabei wird auf der x-Achse die errorID (Fehlerklasse) und auf der y-Achse die Anzahl der Fehler dargestellt. Die Fehlerarten werden nach ihrer Häufigkeit sortiert. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anzahl der Ausfälle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Balkendiagramm erstellen mit x = Error-Klasse und y = Anzahl der Fehler\n",
    "sns.countplot(x='errorID', data=df_error, order=df_error['errorID'].value_counts().index, palette=colors)\n",
    "\n",
    "plt.title('Anzahl der Fehler', fontsize=14, pad=20)\n",
    "plt.ylabel('Anzahl der Fehler', fontsize=12)\n",
    "plt.xlabel('Error ID', fontsize=12)\n",
    "\n",
    "plt.margins(x=0.01)\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ebenso wie bei zuvor dargestellten Anzahl der Fehler können auch die Anzahl der Ausfälle visualisiert werden. Auch hier lässt sich im erstellten Diagramm erkennen, welche Art der Ausfälle wie häufig vorkommt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verteilung des Alters der Maschinen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "import numpy as np\n",
    "\n",
    "# Verteilung des Alters der Maschinen in Prozent\n",
    "age_distribution = df['age'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Alter 13 mit 0% hinzufügen\n",
    "age_distribution = age_distribution.reindex(range(21), fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Balkendiagramm erstellen mit x = Alter und y = Prozent\n",
    "sns.barplot(x=age_distribution.index, y=age_distribution.values, color='b', alpha=0.6)\n",
    "\n",
    "# Density Kurve hinzufügen\n",
    "age_values = df['age'].values\n",
    "kde = gaussian_kde(age_values)\n",
    "age_range = np.linspace(age_values.min(), age_values.max(), 100)\n",
    "age_density = kde(age_range) * 100  # Skalieren auf Prozent\n",
    "\n",
    "plt.plot(age_range, age_density, color='r')\n",
    "\n",
    "plt.title('Verteilung des Alters der Maschinen', fontsize=14, pad=20)\n",
    "plt.ylabel('Prozent', fontsize=12)\n",
    "plt.xlabel('Alter', fontsize=12)\n",
    "\n",
    "plt.margins(x=0.01)\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anhand des erstellten Säulendiagramms lässt sich die Verteilung des Alters der untersuchten Maschinen darstellen. Dabei wird auf der x-Achse das mögliche Alter abgebildet und auf der y-Achse die Verteilung in Prozent. Die meisten Maschinen sind dabei zehn oder 14 Jahre alt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearer Zusammenhang (Regression) mit der Anzahl an Ausfällen und dem Alter der Maschine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zunächst Summierung der Anzahl an Ausfälle pro Alter\n",
    "age_error_count = df.groupby('age').size()\n",
    "\n",
    "# Erstellung einer linearen Darstellung mit Trend (x-Achse = Alter und y = Anzahl an Ausfälle)\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.regplot(x=age_error_count.index, y=age_error_count.values, color='b', scatter_kws={'s': 10})\n",
    "\n",
    "plt.title('Linearer Zusammenhang zwischen Alter und Anzahl an Ausfällen', fontsize=14, pad=20)\n",
    "plt.ylabel('Anzahl an Ausfällen', fontsize=12)\n",
    "plt.xlabel('Alter', fontsize=12)\n",
    "\n",
    "# Setzen der x-Achse, um jedes Jahr als Strich darzustellen\n",
    "plt.xticks(ticks=range(age_error_count.index.min(), age_error_count.index.max() + 1))\n",
    "\n",
    "plt.margins(x=0.01)\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "plt.show()\n",
    "\n",
    "# Berechnen der Regressionsgeraden\n",
    "X = age_error_count.index.values.reshape(-1, 1)\n",
    "y = age_error_count.values\n",
    "\n",
    "# Erstellung des Modell und Training\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Ausgabe der Funktion der Geraden\n",
    "m = model.coef_[0]\n",
    "b = model.intercept_\n",
    "print(f\"Die Funktion der Geraden lautet: y = {m}x + {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Bereich der Datenbeschreibung wird abschließend noch der Zusammenhang zwischen dem Alter der Maschinen und der Anzahl an Ausfällen visualisiert. Zuerst wird die Anzahl der Ausfälle für jedes Alter summiert. Anschließend wird ein Streudiagramm mit einer linearen Regressionsgeraden erstellt, um den Trend zwischen Alter und Ausfällen darzustellen. Die x-Achse zeigt das Alter der Maschinen und die y-Achse die Anzahl der Ausfälle. Ein lineares Regressionsmodell wird trainiert, um die beste Geradengleichung zu berechnen, die den Zusammenhang beschreibt. Die Koeffizienten der Regressionsgeraden werden schließlich ausgegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenvorberarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Umwandlung einzelner Spalten in integer-Werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errorID, failure und comp sind nicht numerisch und müssen umgewandelt werden\n",
    "# Wenn NaN, dann wird 0 eingesetzt, sonst wenn errorID = error1 dann 1, errorID = error2 dann 2, errorID = error3 dann 3,....\n",
    "\n",
    "df['errorID'] = df['errorID'].fillna(0)\n",
    "df['errorID'] = df['errorID'].replace('error1', 1)\n",
    "df['errorID'] = df['errorID'].replace('error2', 2)\n",
    "df['errorID'] = df['errorID'].replace('error3', 3)\n",
    "df['errorID'] = df['errorID'].replace('error4', 4)\n",
    "df['errorID'] = df['errorID'].replace('error5', 5)\n",
    "\n",
    "df['failure'] = df['failure'].fillna(0)\n",
    "df['failure'] = df['failure'].replace('comp1', 1)\n",
    "df['failure'] = df['failure'].replace('comp2', 2)\n",
    "df['failure'] = df['failure'].replace('comp3', 3)\n",
    "df['failure'] = df['failure'].replace('comp4', 4)\n",
    "\n",
    "df['comp'] = df['comp'].fillna(0)\n",
    "df['comp'] = df['comp'].replace('comp1', 1)\n",
    "df['comp'] = df['comp'].replace('comp2', 2)\n",
    "df['comp'] = df['comp'].replace('comp3', 3)\n",
    "df['comp'] = df['comp'].replace('comp4', 4)\n",
    "\n",
    "df['model'] = df['model'].fillna(0)\n",
    "df['model'] = df['model'].replace('model1', 1)\n",
    "df['model'] = df['model'].replace('model2', 2)\n",
    "df['model'] = df['model'].replace('model3', 3)\n",
    "df['model'] = df['model'].replace('model4', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im ersten Schritt der Datenvorverarbeitung werden die Spalten errorID, failure, comp und model umgewandelt, damit sie nur noch numerische Werte enthalten. Zuerst werden alle NaN-Werte in diesen Spalten durch 0 ersetzt. Danach werden die einzelnen Kategorien, wie beispielweise error1, error2, comp1 oder comp2 durch numerische Werte ersetzt: error1 wird zu 1, error2 zu 2 etc. Dies wird für alle genannten Spalten durchgeführt. Dadurch liegen alle Werte in diesen Spalten als numerische Werte vor und können so für die weitere Analyse verwendet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testen auf fehlende Werte\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Missing Value Treatment wird der Dataframe auf fehlende Werte überprüft. Dies ist hier jedoch nicht der Fall, da für alle Variablen der Wert 0 ausgegeben wird. Dadurch ist ebenfalls keine Interpolation oder ähnliches notwendig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausreißer-Analyse (statistisch und dichtebasiert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning (Ausreißer identifizieren und entfernen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaußsche Normalverteilung der Attribute (volt, rotate, pressure, vibration)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Volt\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(df['volt'])\n",
    "plt.title(f'Volt (Skewness: {df[\"volt\"].skew():.2f})')\n",
    "\n",
    "# Rotate\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(df['rotate'])\n",
    "plt.title(f'Rotate (Skewness: {df[\"rotate\"].skew():.2f})')\n",
    "\n",
    "# Pressure\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(df['pressure'])\n",
    "plt.title(f'Pressure (Skewness: {df[\"pressure\"].skew():.2f})')\n",
    "\n",
    "# Vibration\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(df['vibration'])\n",
    "plt.title(f'Vibration (Skewness: {df[\"vibration\"].skew():.2f})')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für das Data Cleaning werden die Verteilungen der vier Attribute volt, rate, pressure und vibration mithilfe von Histogrammen visualisiert. Für jedes Attribut wird zusätzlich die Schiefe berechnet, die im Titel des jeweiligen Plots angezeigt wird. Die Schiefe gibt an, wie asymmetrisch die Verteilung der Daten ist. Ein Wert von 0 deutet auf eine symmetrische Verteilung hin, während positive oder negative Werte auf eine Rechts- bzw. Linksschiefe hinweisen. Diese Visualisierung hilft dabei, die Verteilungen der Attribute besser zu verstehen und zu erkennen, ob sie einer Normalverteilung entsprechen oder signifikante Verzerrungen aufweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistische Ausreißer-Analyse (Mittelwert, Median und Standardabweichung)\n",
    "\n",
    "# Mittelwert\n",
    "\n",
    "print(\"Volt-Mittelwert: \",df['volt'].mean())\n",
    "print(\"Rotation-Mittelwert: \",df['rotate'].mean())\n",
    "print(\"Pressure-Mittelwert: \",df['pressure'].mean())\n",
    "print(\"Vibration-Mittelwert: \",df['vibration'].mean())\n",
    "\n",
    "# Median\n",
    "\n",
    "print(\"Volt-Median: \",df['volt'].median())\n",
    "print(\"Rotation-Median: \",df['rotate'].median())\n",
    "print(\"Pressure-Median: \",df['pressure'].median())\n",
    "print(\"Vibration-Median: \",df['vibration'].median())\n",
    "\n",
    "# Standardabweichung\n",
    "\n",
    "print(\"Volt-Standardabweichung: \",df['volt'].std())\n",
    "print(\"Rotation-Standardabweichung: \",df['rotate'].std())\n",
    "print(\"Pressure-Standardabweichung: \",df['pressure'].std())\n",
    "print(\"Vibration-Standardabweichung: \",df['vibration'].std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Code wird eine statistische Ausreißer-Analyse für die vier Attribute volt, rotate, pressure und vibration durchgeführt. Es werden der Mittelwert, der Median und die Standardabweichung als zentrale Kennwerte berechnet. Der Mittelwert gibt den Durchschnitt der Werte an und kann durch extreme Ausreißer verzerrt werden. Der Median stellt den zentralen Wert der Verteilung dar und ist weniger empfindlich gegenüber Ausreißern. Die Standardabweichung misst die Streuung der Werte um den Mittelwert und hilft, die Variabilität der Daten zu verstehen. Diese Kennzahlen bieten eine Grundlage, um mögliche Ausreißer zu identifizieren, die sich signifikant vom Mittelwert oder Median unterscheiden oder eine hohe Streuung aufweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausreißer-Analyse mit Boxplot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(x=df['volt'])\n",
    "plt.show()\n",
    "\n",
    "# Anzahl der Ausreißer\n",
    "Q1_volt = df['volt'].quantile(0.25)\n",
    "Q3_volt = df['volt'].quantile(0.75)\n",
    "\n",
    "IQR_volt = Q3_volt - Q1_volt\n",
    "\n",
    "print(\"Anzahl der Ausreißer: \", ((df['volt'] < (Q1_volt - 1.5 * IQR_volt)) | (df['volt'] > (Q3_volt + 1.5 * IQR_volt))).sum())\n",
    "\n",
    "# Ausreißer-Analyse für rotate\n",
    "sns.boxplot(x=df['rotate'])\n",
    "plt.show()\n",
    "\n",
    "Q1_rotate = df['rotate'].quantile(0.25)\n",
    "Q3_rotate = df['rotate'].quantile(0.75)\n",
    "IQR_rotate = Q3_rotate - Q1_rotate\n",
    "\n",
    "print(\"Anzahl der Ausreißer (rotate): \", ((df['rotate'] < (Q1_rotate - 1.5 * IQR_rotate)) | (df['rotate'] > (Q3_rotate + 1.5 * IQR_rotate))).sum())\n",
    "\n",
    "# Ausreißer-Analyse für pressure\n",
    "sns.boxplot(x=df['pressure'])\n",
    "plt.show()\n",
    "\n",
    "Q1_pressure = df['pressure'].quantile(0.25)\n",
    "Q3_pressure = df['pressure'].quantile(0.75)\n",
    "IQR_pressure = Q3_pressure - Q1_pressure\n",
    "\n",
    "print(\"Anzahl der Ausreißer (pressure): \", ((df['pressure'] < (Q1_pressure - 1.5 * IQR_pressure)) | (df['pressure'] > (Q3_pressure + 1.5 * IQR_pressure))).sum())\n",
    "\n",
    "# Ausreißer-Analyse für vibration\n",
    "sns.boxplot(x=df['vibration'])\n",
    "plt.show()\n",
    "\n",
    "Q1_vibration = df['vibration'].quantile(0.25)\n",
    "Q3_vibration = df['vibration'].quantile(0.75)\n",
    "IQR_vibration = Q3_vibration - Q1_vibration\n",
    "\n",
    "print(\"Anzahl der Ausreißer (vibration): \", ((df['vibration'] < (Q1_vibration - 1.5 * IQR_vibration)) | (df['vibration'] > (Q3_vibration + 1.5 * IQR_vibration))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ebenfalls durchgeführt wird eine Ausreißer-Analyse für die Attribute volt, rotate, pressure und vibration. Auch hier werden Boxplots erstellt, um visuell Ausreißer zu identifizieren. Anschließend wird der Interquartilsabstand (IQR) berechnet, und Ausreißer werden als Werte außerhalb des Bereichs Q1 - 1.5 * IQR und Q3 + 1.5 * IQR bestimmt. Die Anzahl der Ausreißer für jedes Attribut wird gezählt und angezeigt. Es ergibt sich eine Anzahl von 7480 Ausreißern in der gesamten Analyse und insgesamt die meisten Ausreißer für das Attribut pressure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datennormalisierung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten normalisieren auf Werte zwischen 0 und 1\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[['volt', 'rotate', 'pressure', 'vibration', 'age']] = scaler.fit_transform(df[['volt', 'rotate', 'pressure', 'vibration', 'age']])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe des MinMaxScalers werden die Werte der Attribute volt, rotate, pressure, vibration und age normalisiert. Der MinMaxScaler skalsiert die Werte jedes Attributs auf einen Bereich zwischen 0 und 1. Die normalisierten Werte werden dann in die ursprüngliche DataFrame übernommen und das Ergebnis wird durch df.head() angezeigt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datentransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaußsche Normalverteilung der Attribute (volt, rotate, pressure, vibration)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "# Volt\n",
    "plt.subplot(4, 1, 1)\n",
    "sns.histplot(df['volt'])\n",
    "\n",
    "# Rotate\n",
    "plt.subplot(4, 1, 2)\n",
    "sns.histplot(df['rotate'])\n",
    "\n",
    "# Pressure\n",
    "plt.subplot(4, 1, 3)\n",
    "sns.histplot(df['pressure'])\n",
    "\n",
    "# Vibration\n",
    "plt.subplot(4, 1, 4)\n",
    "sns.histplot(df['vibration'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Datennormalisierung besteht nun auch die Möglichkeit, die Verteilungen der Attribute visuell darzustellen. Dafür werden wieder Histrogramme verwendet, die die normalisierten Daten für die einzelnen Attribute darstellen. Dieser Prozess hilft dabei, die Verteilung der Daten zu überprüfen und mögliche Abweichungen von einer normalen (Gaußschen) Verteilung zu erkennen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering (Volt und Rotate / Pressure und Vibration)\n",
    "\n",
    "# Volt und Rotate\n",
    "df['volt_rotate'] = df['volt'] * df['rotate']\n",
    "\n",
    "# Pressure und Vibration\n",
    "df['pressure_vibration'] = df['pressure'] * df['vibration']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Rahmen der Datenvorverarbeitung wird nun Feature Engineering durchgeführt, um neue Merkmake (Features) zu erstellen, die durch Kombination bestehender Attribute entstehen. Volt und Rotate bilden das Attribut volt_rotate, Pressure und Vibration das Attribut pressure_vibration. Diese neuen Features könnten nützlich sein, um komplexe Zusammenhänge zwischen den bestehenden Variablen zu erfassen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance (der ersten 100.000 Messwerte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = df[['volt', 'rotate', 'pressure', 'vibration', 'age', 'volt_rotate', 'pressure_vibration']].head(100000)\n",
    "y = df['failure'].head(100000)\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "rf_feat_importances = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
    "rf_feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Abschluss im Rahmen der Datenvorverarbeitung wird ein Random Forest Classifier eingesetzt, um die Bedeutung der verschiedenen Merkmale im Hinblick auf die Vorhersage der Zielgröße failure zu bestimmen. Dazu werden die Eingabedaten aus den Attributen volt, rotate, pressure, vibration, age sowie zwei neu berechneten Merkmalen, volt_rotate und pressure_vibration, zusammengestellt. Die Zielvariable ist failure.\n",
    "\n",
    "Der Random Forest wird anschließend mit den ersten 100.000 Datenpunkten trainiert. Nach dem Modelltraining werden die Feature Importances extrahiert, welche die relative Bedeutung jedes Merkmals für die Modellvorhersage darstellen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speicherung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"../data/PdM_merged_preprocessed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die vorverarbeiteten Daten werden als pickle-Datei abgespeichert und für die weiteren Analysen verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomalieerkennung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN-Algorithmus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
